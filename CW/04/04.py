import os
import csv
import uuid
import torch
import requests
from qdrant_client import QdrantClient, models
from langchain_text_splitters import RecursiveCharacterTextSplitter
from transformers import AutoTokenizer, AutoModelForCausalLM

SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
EMBED_API_URL = "https://ws-04.wade0426.me/embed"
LLM_API_URL = "https://ws-02.wade0426.me/v1/chat/completions"
LLM_MODEL = "google/gemma-3-27b-it"

RERANKER_PATH = "/home/tmjh1224/AI/Models/Qwen3-Reranker-0.6B"

COLLECTION_NAME = "CW_04_Hybrid_Rerank"
CHUNK_SIZE = 400
CHUNK_OVERLAP = 100

print("âŒ› æ­£åœ¨è¼‰å…¥ Reranker æ¨¡å‹ (é–‹å•Ÿ FP16 åŠç²¾åº¦æ¨¡å¼)...")
reranker_tokenizer = AutoTokenizer.from_pretrained(RERANKER_PATH, trust_remote_code=True)
reranker_model = AutoModelForCausalLM.from_pretrained(
    RERANKER_PATH, 
    trust_remote_code=True,
    dtype=torch.float16
).eval()

if torch.cuda.is_available():
    reranker_model.to("cuda")

token_false_id = reranker_tokenizer.convert_tokens_to_ids("no")
token_true_id = reranker_tokenizer.convert_tokens_to_ids("yes")

def get_embeddings(texts, task="æª¢ç´¢æ–‡ä»¶"):
    try:
        res = requests.post(EMBED_API_URL, json={
            "texts": texts, "task_description": task, "normalize": True
        }, timeout=30).json()
        return res.get("embeddings", [])
    except: return None

def call_llm(system_prompt, user_prompt):
    try:
        res = requests.post(LLM_API_URL, json={
            "model": LLM_MODEL,
            "messages": [{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}],
            "temperature": 0.1
        }, timeout=60).json()
        return res["choices"][0]["message"]["content"].strip()
    except: return "ç„¡æ³•ç”¢ç”Ÿç­”æ¡ˆ"

@torch.no_grad()
def rerank_docs(query, candidates, initial_points, limit=3):
    """ ä½¿ç”¨ Batching (åˆ†æ‰¹è™•ç†) è§£æ±º 6GB é¡¯å­˜ OOM å•é¡Œ """
    if not candidates: return []
    
    pairs = [f"<Instruct>: æ ¹æ“šæŸ¥è©¢æª¢ç´¢ç›¸é—œæ–‡ä»¶\n<Query>: {query}\n<Document>: {doc}" for doc in candidates]
    
    all_scores = []
    batch_size = 1 # é¡¯å­˜æœ‰é™ï¼Œå¼·åˆ¶ä¸€æ‰¹ä¸€ç­†
    
    for i in range(0, len(pairs), batch_size):
        batch_pairs = pairs[i : i + batch_size]
        inputs = reranker_tokenizer(
            batch_pairs, padding=True, truncation=True, return_tensors="pt", max_length=2048
        )
        for k in inputs: inputs[k] = inputs[k].to(reranker_model.device)
        
        logits = reranker_model(**inputs).logits[:, -1, :]
        batch_scores = torch.stack([logits[:, token_false_id], logits[:, token_true_id]], dim=1)
        batch_scores = torch.nn.functional.softmax(batch_scores, dim=1)[:, 1].tolist()
        all_scores.extend(batch_scores)
        
        del inputs, logits
        torch.cuda.empty_cache()

    combined = []
    for i in range(len(candidates)):
        combined.append({
            "text": candidates[i],
            "score": all_scores[i],
            "source": initial_points[i].payload.get("source", "æœªçŸ¥")
        })
    combined.sort(key=lambda x: x["score"], reverse=True)
    return combined[:limit]

def main():
    # å¢åŠ  timeout=60 è§£æ±º Qdrant ReadTimeout å•é¡Œ
    client = QdrantClient("localhost", port=6333, timeout=60)
    
    # 1. åˆå§‹åŒ– VDB
    print(f"ğŸš€ åˆå§‹åŒ–é›†åˆ: {COLLECTION_NAME}")
    sample_emb = get_embeddings(["æ¸¬è©¦ç¶­åº¦"])
    dim = len(sample_emb[0]) if sample_emb else 4096
    if client.collection_exists(COLLECTION_NAME): client.delete_collection(COLLECTION_NAME)
    client.create_collection(
        collection_name=COLLECTION_NAME,
        vectors_config={"dense": models.VectorParams(size=dim, distance=models.Distance.COSINE)},
        sparse_vectors_config={"sparse": models.SparseVectorParams(modifier=models.Modifier.IDF)}
    )

    splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)
    for i in range(1, 6):
        path = os.path.join(SCRIPT_DIR, f"data_0{i}.txt")
        if os.path.exists(path):
            with open(path, "r", encoding="utf-8") as f:
                chunks = splitter.split_text(f.read())
                embs = get_embeddings(chunks)
                if embs:
                    points = [
                        models.PointStruct(
                            id=uuid.uuid4().hex,
                            vector={"dense": emb, "sparse": models.Document(text=chunk, model="Qdrant/bm25")},
                            payload={"text": chunk, "source": f"data_0{i}.txt"}
                        ) for chunk, emb in zip(chunks, embs)
                    ]
                    client.upsert(COLLECTION_NAME, points)
    print("çŸ¥è­˜åº«ç´¢å¼•å»ºç«‹å®Œæˆ (Hybrid)")

    input_csv = os.path.join(SCRIPT_DIR, "questions.csv")
    if not os.path.exists(input_csv):
        print("æ‰¾ä¸åˆ°åŸå§‹ questions.csv æª”æ¡ˆ"); return

    with open(input_csv, "r", encoding="utf-8-sig") as f:
        reader = csv.DictReader(f)
        rows = list(reader)

    print(f"é–‹å§‹è™•ç† {len(rows)} å€‹å•é¡Œ (Hybrid Search + Rerank)...")
    for r in rows:
        user_q = r.get('é¡Œç›®') or r.get('questions')
        
        # A. Hybrid Search
        q_emb = get_embeddings([user_q], task="æŸ¥è©¢")[0]
        search_res = client.query_points(
            collection_name=COLLECTION_NAME,
            prefetch=[
                models.Prefetch(query=models.Document(text=user_q, model="Qdrant/bm25"), using="sparse", limit=15),
                models.Prefetch(query=q_emb, using="dense", limit=15),
            ],
            query=models.FusionQuery(fusion=models.Fusion.RRF),
            limit=15
        ).points

        # B. Reranking (åŠ å…¥åˆ†æ‰¹è™•ç†ä»¥é˜² Timeout/OOM)
        candidates = [p.payload["text"] for p in search_res]
        reranked_results = rerank_docs(user_q, candidates, search_res, limit=3)
        
        context = "\n".join([item["text"] for item in reranked_results])
        top_source = reranked_results[0]["source"] if reranked_results else "æœªçŸ¥"

        ans_sys = "ä½ æ˜¯ä¸€å€‹å°ˆæ¥­åŠ©æ‰‹ï¼Œè«‹æ ¹æ“šåƒè€ƒè³‡æ–™ç°¡çŸ­å›ç­”å•é¡Œã€‚è‹¥åƒè€ƒè³‡æ–™ä¸­æ²’æœ‰æåˆ°ï¼Œè«‹å›ç­”ä¸çŸ¥é“ã€‚"
        ans_usr = f"åƒè€ƒè³‡æ–™ï¼š\n{context}\n\nå•é¡Œï¼š{user_q}"
        answer = call_llm(ans_sys, ans_usr)

        # å¡«å…¥è€å¸«è¦æ±‚çš„ä¸­æ–‡æ¬„ä½
        r["æ¨™æº–ç­”æ¡ˆ"] = answer
        r["ä¾†æºæ–‡ä»¶"] = top_source
        print(f"  - å®Œæˆ: {user_q[:15]}...")

    output_path = os.path.join(SCRIPT_DIR, "questions_answer_final.csv")
    with open(output_path, "w", encoding="utf-8-sig", newline="") as f:
        writer = csv.DictWriter(f, fieldnames=rows[0].keys())
        writer.writeheader()
        writer.writerows(rows)
    
    print(f"\nè«‹æª¢æŸ¥æª”æ¡ˆ: {output_path}")

if __name__ == "__main__":
    main()