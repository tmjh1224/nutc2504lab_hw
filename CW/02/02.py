import os
import requests
import re
from bs4 import BeautifulSoup
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct

# ================= 1. è¨­å®šèˆ‡åˆå§‹åŒ– =================
API_EMBED_URL = "https://ws-04.wade0426.me/embed"
API_SIMILARITY_URL = "https://ws-04.wade0426.me/similarity"
QDRANT_URL = "http://localhost:6333"

# åˆå§‹åŒ– Qdrant å®¢æˆ¶ç«¯
try:
    q_client = QdrantClient(url=QDRANT_URL)
    print("å·²æˆåŠŸé€£æ¥è‡³ Qdrant VDB")
except Exception as e:
    print(f"ç„¡æ³•é€£æ¥ Qdrant: {e}")

# ================= 2. å·¥å…·å‡½æ•¸å°è£ =================

def get_embeddings(texts):
    """å–å¾—å‘é‡"""
    response = requests.post(API_EMBED_URL, json={
        "texts": texts, "task_description": "æª¢ç´¢æŠ€è¡“æ–‡ä»¶", "normalize": True
    })
    return response.json().get("embeddings", [])

def get_similarity(query, documents):
    """è¨ˆç®—ç›¸ä¼¼åº¦åˆ†æ•¸"""
    response = requests.post(API_SIMILARITY_URL, json={
        "queries": [query], "documents": documents
    })
    return response.json().get("similarity", [[]])[0]

# --- åˆ‡å¡Šé‚è¼¯ ---
def fixed_size_chunking(text, size=300):
    return [text[i:i+size] for i in range(0, len(text), size)]

def sliding_window_chunking(text, size=300, overlap=100):
    chunks = []
    step = size - overlap
    for i in range(0, len(text) - overlap, step):
        chunks.append(text[i:i+size])
    return chunks

# --- è¡¨æ ¼è™•ç† ---
def process_table(file_path):
    if not os.path.exists(file_path): return ""
    if file_path.endswith('.html'):
        with open(file_path, 'r', encoding='utf-8') as f:
            soup = BeautifulSoup(f, 'html.parser')
            return "\n".join([" | ".join([td.get_text(strip=True) for td in tr.find_all(['td', 'th'])]) for tr in soup.find_all('tr')])
    else:
        with open(file_path, 'r', encoding='utf-8') as f: return f.read()

# ================= 3. ä¸»æµç¨‹ =================

def main():
    # 1. è®€å–èˆ‡åˆ‡å¡Š
    with open("text.txt", "r", encoding="utf-8") as f:
        content = f.read()
    
    chunks_f = fixed_size_chunking(content, 300)
    chunks_s = sliding_window_chunking(content, 300, 100)

    # 2. åµŒå…¥èˆ‡å­˜å…¥ Qdrant
    print("\n--- æ­£åœ¨å­˜å…¥ Qdrant VDB ---")
    all_chunks = chunks_f + chunks_s
    vectors = get_embeddings(all_chunks)
    
    if vectors:
        col_name = "hw02_collection"
        q_client.recreate_collection(
            collection_name=col_name,
            vectors_config=VectorParams(size=len(vectors[0]), distance=Distance.COSINE)
        )
        points = [PointStruct(id=i, vector=v, payload={"text": c}) for i, (v, c) in enumerate(zip(vectors, all_chunks))]
        q_client.upsert(col_name, points)
        print(f"âœ… æˆåŠŸå°‡ {len(points)} å€‹ Points å­˜å…¥ Dashboard")

    # 3. å¬å›æ¯”è¼ƒ
    query = "Graph RAG èˆ‡å‚³çµ± RAG çš„å·®ç•°æ˜¯ä»€éº¼ï¼Ÿ"
    score_f = get_similarity(query, chunks_f)
    score_s = get_similarity(query, chunks_s)

    max_f = max(score_f) if score_f else 0
    max_s = max(score_s) if score_s else 0

    print(f"\nğŸ” æ¸¬è©¦å•é¡Œ: {query}")
    print(f"ğŸ“Š å›ºå®šåˆ‡å¡Šæœ€é«˜åˆ†: {max_f:.4f}")
    print(f"ğŸ“Š æ»‘å‹•è¦–çª—æœ€é«˜åˆ†: {max_s:.4f}")
    print(f"ğŸ† çµæœ: {'æ»‘å‹•è¦–çª—ç²å‹' if max_s > max_f else 'å›ºå®šå¤§å°ç²å‹'}")

    # 4. è¡¨æ ¼è™•ç† (Step 6)
    html_tab = process_table("table_html.html")
    md_tab = process_table("table_txt.md")
    print(f"\nğŸ“ è¡¨æ ¼è™•ç†å®Œæˆ: HTML({len(html_tab)}å­—), MD({len(md_tab)}å­—)")
    print("\n--- HTML è¡¨æ ¼è½‰æ›çµæœ ---")
    print(html_tab) 

    print("\n--- Markdown è¡¨æ ¼è½‰æ›çµæœ ---")
    print(md_tab)

if __name__ == "__main__":
    main()
    