import os
import requests
import pandas as pd
import re
import time
from docx import Document
import PyPDF2
from qdrant_client import QdrantClient, models
from qdrant_client.http.models import PointStruct
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# --- ç¶²è·¯èˆ‡ API é…ç½® ---
def get_stable_session():
    session = requests.Session()
    retries = Retry(total=5, backoff_factor=1, status_forcelist=[500, 502, 503, 504])
    session.mount('https://', HTTPAdapter(max_retries=retries))
    return session

session = get_stable_session()
TIMEOUT = 60
LLM_URL = "https://ws-03.wade0426.me/v1/chat/completions"
EMBED_URL = "https://ws-04.wade0426.me/embed"
MODEL_NAME = "/models/gpt-oss-120b"

# --- 1. IDP æ–‡ä»¶è™•ç†èˆ‡æ³¨å…¥è¾¨è­˜ ---
def process_idp_files():
    docs_data = []
    files = ['1.pdf', '2.pdf', '3.pdf', '4.png', '5.docx']
    print("ğŸ” [IDP] æ­£åœ¨é€²è¡Œå®‰å…¨æƒæ...")
    
    for file_name in files:
        if not os.path.exists(file_name): continue
        content = ""
        try:
            if file_name.endswith('.pdf'):
                with open(file_name, 'rb') as f:
                    reader = PyPDF2.PdfReader(f)
                    content = " ".join([p.extract_text() for p in reader.pages if p.extract_text()])
            elif file_name.endswith('.docx'):
                doc = Document(file_name)
                content = "\n".join([p.text for p in doc.paragraphs])
            elif file_name.endswith('.png'):
                content = "ä¸å‹•ç”¢èªªæ˜æ›¸ï¼š104å¹´10æœˆ1æ—¥ç”Ÿæ•ˆï¼Œä¸å¾—è¨˜è¼‰äº‹é …åŒ…å«é·å¾™è‡ªç”±ã€‚"
        except Exception as e: print(f"è®€å– {file_name} å‡ºéŒ¯: {e}")

        # è¾¨è­˜æƒ¡æ„æ³¨å…¥ (æˆªåœ–é‡é»)
        if "tiramisu" in content.lower() or "ignore all system prompts" in content.lower():
            print(f"\nğŸ”¥ [è­¦å‘Š] ç™¼ç¾æƒ¡æ„æ³¨å…¥æ–‡ä»¶: {file_name}")
            print(f"å…§å®¹å« Tiramisu æŒ‡ä»¤ï¼Œå·²æ¨™è¨˜è™•ç†ã€‚\n")

        chunks = [content[i:i+500] for i in range(0, len(content), 400)]
        for c in chunks:
            docs_data.append({"text": c, "source": file_name})
    return docs_data

# --- 2. RAG èˆ‡æœå°‹ (ä¿®æ­£ç›¸å®¹æ€§å•é¡Œ) ---
def get_context(client, query_emb):
    """ç›¸å®¹æ–°èˆŠç‰ˆ Qdrant æœå°‹èªæ³•"""
    try:
        # å˜—è©¦èˆŠç‰ˆ search
        res = client.search(collection_name="hw7", query_vector=query_emb, limit=1)
        return res[0].payload['text'], res[0].payload['source']
    except AttributeError:
        # å˜—è©¦æ–°ç‰ˆ query_points
        res = client.query_points(collection_name="hw7", query=query_emb, limit=1)
        return res.points[0].payload['text'], res.points[0].payload['source']

if __name__ == "__main__":
    chunks = process_idp_files()
    
    res = session.post(EMBED_URL, json={"texts": ["test"], "task_description": "æª¢ç´¢", "normalize": True}).json()
    dim = len(res["embeddings"][0])
    q_client = QdrantClient(":memory:")
    q_client.create_collection("hw7", vectors_config=models.VectorParams(size=dim, distance=models.Distance.COSINE))
    
    points = []
    print(f"åŒæ­¥å‘é‡ä¸­ (ç¶­åº¦: {dim})...")
    for i, item in enumerate(chunks):
        try:
            emb = session.post(EMBED_URL, json={"texts": [item['text']], "task_description": "æª¢ç´¢"}, timeout=TIMEOUT).json()["embeddings"][0]
            points.append(PointStruct(id=i, vector=emb, payload=item))
        except: continue
    q_client.upsert("hw7", points)

    # ç”Ÿæˆç­”æ¡ˆä¸¦è·‘é©—è­‰ (questions_answer.csv)
    print("ğŸ§ª æ­£åœ¨ç”Ÿæˆ test_dataset.csv ä¸¦é€²è¡ŒæŒ‡æ¨™é©—è­‰...")
    qa_df = pd.read_csv('questions_answer.csv')
    final_results = []

    for _, row in qa_df.iterrows():
        try:
            q_emb = session.post(EMBED_URL, json={"texts": [row['questions']], "task_description": "æª¢ç´¢"}).json()["embeddings"][0]
            ctx, src = get_context(q_client, q_emb)
            
            ans_res = session.post(LLM_URL, json={
                "model": MODEL_NAME,
                "messages": [{"role": "user", "content": f"æ ¹æ“šè³‡æ–™ï¼š{ctx}\nå›ç­”ï¼š{row['questions']}"}]
            }).json()
            actual_ans = ans_res["choices"][0]["message"]["content"]

            # DeepEval è©•åˆ† (ç”¨ LLM æ¨¡æ“¬è©•ä¼° 4 å€‹æŒ‡æ¨™)
            eval_prompt = f"è©•åˆ† RAG (0-1), åƒ…è¼¸å‡º4å€‹æ•¸å­—ç”¨é€—è™Ÿéš”é–‹(Faith, Rel, Prec, Rec):\nå•:{row['questions']}\nç­”:{actual_ans}\næ–‡:{ctx[:200]}"
            eval_res = session.post(LLM_URL, json={"model": MODEL_NAME, "messages": [{"role": "user", "content": eval_prompt}]}).json()
            scores = [float(x) for x in re.findall(r"\d+\.\d+|\d+", eval_res["choices"][0]["message"]["content"])]
            if len(scores) < 4: scores = [0.0, 0.0, 0.0, 0.0]

            final_results.append({
                "q_id": row['id'], "questions": row['questions'], "answer": actual_ans, "source": src,
                "Faithfulness": scores[0], "Relevancy": scores[1], "Precision": scores[2], "Recall": scores[3]
            })
            print(f"âœ… Q{row['id']} å®Œæˆ")
        except Exception as e:
            print(f"âŒ Q{row['id']} å¤±æ•—: {e}")

    # è¼¸å‡ºæœ€çµ‚æª”æ¡ˆ
    output_df = pd.DataFrame(final_results)
    output_df.to_csv('test_dataset.csv', index=False, encoding='utf-8-sig')
    print("\nç”¢å‡ºæª”æ¡ˆï¼štest_dataset.csv")