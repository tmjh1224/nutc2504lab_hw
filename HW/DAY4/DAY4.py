import os
import operator
import base64
import requests
import json
from typing import Annotated, List, Dict, Union, TypedDict
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage, BaseMessage
from langchain_core.prompts import ChatPromptTemplate
from langgraph.graph import StateGraph, END
from playwright.sync_api import sync_playwright

# 1. é…ç½®èˆ‡å·¥å…·å‡½æ•¸

# æ¨¡æ“¬å…¨åŸŸå¿«å– (ç°¡å–®å­—å…¸å¯¦ä½œ)
ANSWER_CACHE = {}

# 1. LLM: ç”¨æ–¼é‚è¼¯åˆ¤æ–·ã€è¦åŠƒèˆ‡ç”Ÿæˆå›ç­” (ä½¿ç”¨ ws-03 / gpt-oss-120b)
llm_main = ChatOpenAI(
    base_url="https://ws-03.wade0426.me/v1",
    api_key="EMPTY", # å·¥ä½œåŠç’°å¢ƒé€šå¸¸ä¸éœ€è¦ Keyï¼Œæˆ–è«‹è‡ªè¡Œå¡«å…¥
    model="/models/gpt-oss-120b",
    temperature=0
)

# 2. VLM: ç”¨æ–¼è¦–è¦ºè®€å–ç¶²é æˆªåœ– (ä½¿ç”¨ ws-02 / gemma-3-27b-it)
llm_vlm = ChatOpenAI(
    base_url="https://ws-02.wade0426.me/v1",
    api_key="EMPTY",
    model="google/gemma-3-27b-it",
    temperature=0
)

# 3. SearXNG: æœå°‹å¼•æ“
SEARXNG_URL = "https://ws-searxng.huannago.com/search"


def search_searxng(query: str, limit: int = 3) -> List[Dict]:
    """åŸ·è¡Œ SearXNG æœå°‹"""
    print(f"ğŸ” [Search] æ­£åœ¨æœå°‹: {query}")
    params = {"q": query, "format": "json", "language": "zh-TW"}
    try:
        response = requests.get(SEARXNG_URL, params=params, timeout=10)
        if response.status_code == 200:
            results = response.json().get('results', [])
            valid_results = [r for r in results if 'url' in r]
            return valid_results[:limit]
    except Exception as e:
        print(f"âŒ æœå°‹éŒ¯èª¤: {e}")
    return []

def vlm_read_website(url: str, title: str = "ç¶²é å…§å®¹") -> str:
    """ä½¿ç”¨ Playwright æ»¾å‹•æˆªåœ– + VLM åˆ†æ"""
    print(f"ğŸ“¸ [VLM] å•Ÿå‹•è¦–è¦ºé–±è®€: {url}")
    
    # å…§éƒ¨å‡½æ•¸ï¼šæˆªåœ–
    def capture_screenshots(target_url):
        screenshots = []
        try:
            with sync_playwright() as p:
                browser = p.chromium.launch(headless=True, args=["--disable-blink-features=AutomationControlled"])
                page = browser.new_page(viewport={'width': 1280, 'height': 1200})
                page.goto(target_url, wait_until="domcontentloaded", timeout=20000)
                page.wait_for_timeout(2000)
                
                # ç°¡å–®æ»¾å‹•ä¸¦æˆªåœ–
                screenshots.append(base64.b64encode(page.screenshot()).decode('utf-8'))
                page.evaluate("window.scrollBy(0, 1000)")
                page.wait_for_timeout(1000)
                screenshots.append(base64.b64encode(page.screenshot()).decode('utf-8'))
                browser.close()
        except Exception as e:
            print(f"âŒ æˆªåœ–å¤±æ•—: {e}")
        return screenshots

    images = capture_screenshots(url)
    if not images: return "ç„¡æ³•è®€å–ç¶²é ã€‚"

    msg_content = [{"type": "text", "text": f"é€™æ˜¯ç¶²é  '{title}' çš„æˆªåœ–ã€‚è«‹æ‘˜è¦æ ¸å¿ƒå…§å®¹ï¼Œé—œæ³¨æ•¸æ“šèˆ‡äº‹å¯¦ã€‚"}]
    for img in images:
        msg_content.append({"type": "image_url", "image_url": {"url": f"data:image/png;base64,{img}"}})
    
    try:
        response = llm_vlm.invoke([HumanMessage(content=msg_content)])
        return response.content
    except Exception as e:
        return f"VLM åˆ†æå¤±æ•—: {e}"

# 2. å®šç¾© Graph State (ç‹€æ…‹)

class AgentState(TypedDict):
    question: str                   # åŸå§‹å•é¡Œ
    messages: List[BaseMessage]     # å°è©±æ­·å²
    knowledge_base: str             # æ”¶é›†åˆ°çš„è³‡è¨Šæ‘˜è¦
    search_queries: List[str]       # ç”Ÿæˆçš„é—œéµå­—
    loop_count: int                 # å¾ªç’°æ¬¡æ•¸
    final_answer: str               # æœ€çµ‚ç­”æ¡ˆ
    decision: str                   # æ±ºç­–çµæœ (YES/NO)

# 3. å®šç¾© Nodes (ç¯€é»é‚è¼¯)

def check_cache_node(state: AgentState):
    """å¿«å–æª¢æŸ¥ç¯€é»"""
    question = state["question"]
    print(f"\nğŸš€ [Check Cache] æª¢æŸ¥å¿«å–: {question}")
    
    if question in ANSWER_CACHE:
        print("âœ… å¿«å–å‘½ä¸­ï¼ç›´æ¥è¿”å›çµæœã€‚")
        return {"final_answer": ANSWER_CACHE[question], "knowledge_base": "From Cache"}
    
    return {"knowledge_base": state.get("knowledge_base", "")}

def planner_node(state: AgentState):
    """æ±ºç­–ç¯€é» (ä½¿ç”¨ llm_main)"""
    question = state["question"]
    kb = state.get("knowledge_base", "")
    loop = state.get("loop_count", 0)
    
    print(f"ğŸ§  [Planner] è©•ä¼°è³‡è¨Šå……è¶³åº¦ (Loop: {loop})")
    
    if loop >= 3:
        print("âš ï¸ é”åˆ°æœ€å¤§å¾ªç’°æ¬¡æ•¸ï¼Œå¼·åˆ¶å›ç­”ã€‚")
        return {"decision": "sufficient"}

    if not kb:
        return {"decision": "insufficient"}

    prompt = f"""
    ä½ æ˜¯ç ”ç©¶è¦åŠƒå“¡ã€‚
    ä½¿ç”¨è€…çš„å•é¡Œ: "{question}"
    ç›®å‰æ”¶é›†åˆ°çš„è³‡è¨Š:
    ---
    {kb}
    ---
    è«‹å•ç›®å‰çš„è³‡è¨Šæ˜¯å¦è¶³ä»¥è©³ç´°å›ç­”ä½¿ç”¨è€…çš„å•é¡Œï¼Ÿ
    å¦‚æœè¶³å¤ ï¼Œè«‹å›ç­” "YES"ã€‚
    å¦‚æœä¸è¶³ï¼Œè«‹å›ç­” "NO"ã€‚
    åªå›ç­” YES æˆ– NOï¼Œä¸è¦æœ‰å…¶ä»–å»¢è©±ã€‚
    """
    response = llm_main.invoke(prompt).content.strip().upper()
    
    if "YES" in response:
        return {"decision": "sufficient"}
    else:
        return {"decision": "insufficient"}

def query_gen_node(state: AgentState):
    """é—œéµå­—ç”Ÿæˆç¯€é» (ä½¿ç”¨ llm_main)"""
    question = state["question"]
    kb = state.get("knowledge_base", "")
    
    print("âœï¸ [Query Gen] ç”Ÿæˆæœå°‹é—œéµå­—...")
    
    prompt = f"""
    ä½¿ç”¨è€…çš„å•é¡Œ: "{question}"
    ç›®å‰å·²çŸ¥è³‡è¨Š: "{kb}"
    
    è«‹ç”Ÿæˆ 1 å€‹æœ€é©åˆçš„æœå°‹é—œéµå­—ä¾†å°‹æ‰¾ç¼ºå°‘çš„è³‡è¨Šã€‚
    ç›´æ¥è¼¸å‡ºé—œéµå­—å³å¯ï¼Œä¸è¦åŠ å¼•è™Ÿæˆ–è§£é‡‹ã€‚
    """
    query = llm_main.invoke(prompt).content.strip()
    return {"search_queries": [query], "loop_count": state["loop_count"] + 1}

def search_tool_node(state: AgentState):
    """æª¢ç´¢èˆ‡è™•ç†ç¯€é» (Search + VLM)"""
    queries = state["search_queries"]
    current_kb = state.get("knowledge_base", "")
    query = queries[-1]
    
    results = search_searxng(query, limit=1)
    new_info = ""
    
    if results:
        target = results[0]
        url = target.get("url")
        title = target.get("title")
        snippet = target.get("content", "")
        
        print(f"ğŸŒ [Search Tool] æ‰¾åˆ°é€£çµ: {title} ({url})")
        
        vlm_content = vlm_read_website(url, title)
        
        new_info = f"\n[ä¾†æº: {title}]\næœå°‹æ‘˜è¦: {snippet}\nç¶²é è©³æƒ…: {vlm_content}\n"
    else:
        new_info = f"\n[æœå°‹å¤±æ•—] é—œéµå­— '{query}' æ²’æœ‰æ‰¾åˆ°çµæœã€‚\n"

    print("ğŸ“¥ [Search Tool] æ›´æ–°çŸ¥è­˜åº«")
    return {"knowledge_base": current_kb + new_info}
def final_answer_node(state: AgentState):
    """æœ€çµ‚å›ç­”ç¯€é» (ä½¿ç”¨ llm_main)"""
    question = state["question"]
    kb = state["knowledge_base"]
    
    print("ğŸ“ [Final Answer] ç”Ÿæˆæœ€çµ‚å ±å‘Š...")
    
    prompt = f"""
    è«‹æ ¹æ“šä»¥ä¸‹æ”¶é›†åˆ°çš„è³‡è¨Šï¼Œå›ç­”ä½¿ç”¨è€…çš„å•é¡Œã€‚
    
    å•é¡Œ: {question}
    
    æ”¶é›†è³‡è¨Š:
    {kb}
    
    è«‹ä»¥ç¹é«”ä¸­æ–‡ï¼Œå°ˆæ¥­ä¸”æ¢ç†åˆ†æ˜åœ°å›ç­”ã€‚
    """
    answer = llm_main.invoke(prompt).content
    
    ANSWER_CACHE[question] = answer
    return {"final_answer": answer}

# 4. æ§‹å»º Graph

workflow = StateGraph(AgentState)

workflow.add_node("check_cache", check_cache_node)
workflow.add_node("planner", planner_node)
workflow.add_node("query_gen", query_gen_node)
workflow.add_node("search_tool", search_tool_node)
workflow.add_node("final_answer", final_answer_node)

workflow.set_entry_point("check_cache")

def check_cache_router(state: AgentState):
    if state.get("final_answer"):
        return "end"
    return "planner"

workflow.add_conditional_edges(
    "check_cache",
    check_cache_router,
    {"end": END, "planner": "planner"}
)

def planner_router(state: AgentState):
    # æ ¹æ“š planner_node å¯«å…¥çš„ decision é€²è¡Œè·¯ç”±
    if state.get("decision") == "sufficient":
        return "final_answer"
    return "query_gen"

workflow.add_conditional_edges(
    "planner",
    planner_router,
    {"final_answer": "final_answer", "query_gen": "query_gen"}
)

workflow.add_edge("query_gen", "search_tool")
workflow.add_edge("search_tool", "planner")
workflow.add_edge("final_answer", END)

app = workflow.compile()

if __name__ == "__main__":
    print("è‡ªå‹•æŸ¥è­‰AI\n")
    
    # user_question = "NVIDIA GTC 2026 çš„èˆ‰è¾¦æ—¥æœŸèˆ‡åœ°é»æ˜¯ä»€éº¼ï¼Ÿ"
    print("è«‹è¼¸å…¥æ‚¨æƒ³æŸ¥è©¢çš„äº‹ç‰©")
    user_question = input()
    inputs = {
        "question": user_question,
        "loop_count": 0,
        "knowledge_base": "",
        "messages": [],
        "search_queries": []
    }
    
    for output in app.stream(inputs):
        for key, value in output.items():
            print(f"ğŸ”¹ ç¯€é»å®Œæˆ: {key}")

    print("\n" + "="*30)
    if user_question in ANSWER_CACHE:
        print(f"ğŸ“ æœ€çµ‚å›ç­”:\n{ANSWER_CACHE[user_question]}")
    else:
        print("âŒ æœªèƒ½ç”Ÿæˆå›ç­”ã€‚") 